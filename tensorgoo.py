# -*- coding: utf-8 -*-
"""tensorgoo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dEIq__G42fScqRjt3ctdfGBOkIJJi9Bp
"""

!pip install datasets

!pip install rouge_score

from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset
import torch
import evaluate

!pip install evaluate

!pip install datasets

!pip install rouge_score

!pip install evaluate

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset
import evaluate

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)

# Load and preprocess dataset
dataset = load_dataset("cnn_dailymail", '3.0.0', split='train[:1%]')

def preprocess_function(examples):
    inputs = examples['article']
    targets = examples['highlights']

    model_inputs = tokenizer(inputs, max_length=1024, padding="max_length", truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, padding="max_length", truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=1,
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# Train the model
trainer.train()

def generate_summary(text, model, tokenizer):
    inputs = tokenizer(text, return_tensors="pt", max_length=1024, truncation=True, padding="max_length")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

user_input = input("Enter text for summarization: ")
summary = generate_summary(user_input, model, tokenizer)
print(f"Summary: {summary}")

# Evaluate fine-tuned model
predictions, references = evaluate_model(model, tokenizer, tokenized_dataset)
# Load pre-trained model
pretrained_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)
# Evaluate pre-trained model
pretrain_predictions, pretrain_references = evaluate_model(pretrained_model, tokenizer, tokenized_dataset)
rouge_metric = evaluate.load('rouge')
def compute_rouge(predictions, references):
    results = rouge_metric.compute(predictions=predictions, references=references)
    return results
# Compute ROUGE scores
fine_tuned_rouge = compute_rouge(predictions, references)
pretrain_rouge = compute_rouge(pretrain_predictions, pretrain_references)
print("Fine-tuned model ROUGE scores:", fine_tuned_rouge)
print("Pre-trained model ROUGE scores:", pretrain_rouge)